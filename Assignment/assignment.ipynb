{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCxDg56_AWNB"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/Business-Intelligence-and-Analytics/blob/master/Assignment/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rGZ6kbilZZYM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import statistics\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, recall_score, precision_score\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler, PolynomialFeatures\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYGuKo04mVkr"
      },
      "source": [
        "<div>\n",
        "<td>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Logo_Universit%C3%A9_de_Lausanne.svg/2000px-Logo_Universit%C3%A9_de_Lausanne.svg.png\" style=\"padding-right:10px;width:240px;float:left\"/></td>\n",
        "<h2 style=\"white-space: nowrap\">Business Intelligence and Analytics Personal Assignment</h2></td>\n",
        "<hr style=\"clear:both\">\n",
        "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcSQ4nreg3ed"
      },
      "source": [
        "# IMPORTANT\n",
        "\n",
        "Question 1: Before starting this assignment, please indicate whether this course is worth 4.5 or 6 credits to you. Please answer truthfully, as checks will be made afterwards. This question is only intended to facilitate the calculation of final points.\n",
        "\n",
        "> TODO: 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLO9775Ol1KD"
      },
      "source": [
        "# Part 1: Climate Analytics - Rice vs Wheat in a Changing World\n",
        "\n",
        "## Context\n",
        "\n",
        "You work as a junior data analyst at SerriFleur, an international NGO focused on sustainable agriculture in the face of climate change. Your team is investigating how climate conditions and farming practices affect the yields of rice and wheat — two of the most critical staple crops globally.\n",
        "\n",
        "You’ve just received a massive dataset from the research division: \"Hydroponix.csv\". Your task is to extract insights, build models, and tell a compelling story from the data. You can find the dataset here: https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/refs/heads/master/data/Hydroponix.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPZrAcSrv32W"
      },
      "source": [
        "Columns Overview:\n",
        "\n",
        "* `Year` : Year of the observation.\n",
        "\n",
        "* `Country` : Country where the data was collected.\n",
        "\n",
        "* `Region` : Specific region within the country.\n",
        "\n",
        "* `Crop_Type` : Type of agricultural crop observed.\n",
        "\n",
        "* `Average_Temperature` : Annual average temperature in degrees Celsius.\n",
        "\n",
        "* `Total_Precipitation` : Total annual precipitation in millimeters.\n",
        "\n",
        "* `CO2_Emissions` : CO₂ emissions in million tons.\n",
        "\n",
        "* `Crop_Yield` : Agricultural yield in tons per hectare.\n",
        "\n",
        "* `Extreme_Weather_Events` : Number of recorded extreme weather events.\n",
        "\n",
        "* `Irrigation_Access` : Percentage of cultivated land with access to irrigation.\n",
        "\n",
        "* `Pesticide_Use` : Pesticide use in kilograms per hectare.\n",
        "\n",
        "* `Fertilizer_Use` : Fertilizer use in kilograms per hectare.\n",
        "\n",
        "* `Soil_Health_Index` : Soil quality index (out of 100).\n",
        "\n",
        "* `Economic_Impact` : Economic impact in million USD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "d9GkXOmNolo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501a7c06-f25d-45ee-8ac4-64cff4a31e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Year    Country              Region  Crop_Type  Average_Temperature  \\\n",
            "0     2001      India         West Bengal       Corn                 1.55   \n",
            "1     2024      China               North       Corn                 3.23   \n",
            "2     2001     France       Ile-de-France      Wheat                21.11   \n",
            "3     2001     Canada            Prairies     Coffee                27.85   \n",
            "4     1998      India          Tamil Nadu  Sugarcane                 2.19   \n",
            "...    ...        ...                 ...        ...                  ...   \n",
            "9995  2022     France  Nouvelle-Aquitaine     Cotton                30.48   \n",
            "9996  1999  Australia          Queensland   Soybeans                 9.53   \n",
            "9997  2000  Argentina           Patagonia     Coffee                31.92   \n",
            "9998  1996     Brazil           Southeast   Soybeans                13.95   \n",
            "9999  2015      China               South       Corn                11.78   \n",
            "\n",
            "      Total_Precipitation  CO2_Emissions  Crop_Yield  Irrigation_Access  \\\n",
            "0                  447.06          15.22       1.737              14.54   \n",
            "1                 2913.57          29.82       1.737              11.05   \n",
            "2                 1301.74          25.75       1.719              84.42   \n",
            "3                 1154.36          13.91       3.890              94.06   \n",
            "4                 1627.48          11.81       1.080              95.75   \n",
            "...                   ...            ...         ...                ...   \n",
            "9995               685.93          17.64       3.033              27.56   \n",
            "9996              2560.38          10.68       2.560              77.02   \n",
            "9997               357.76          26.01       1.161              78.53   \n",
            "9998              1549.52          17.31       3.348              42.65   \n",
            "9999              1676.25           5.34       3.710              46.41   \n",
            "\n",
            "      Pesticide_Use  Fertilizer_Use  Soil_Health_Index  Economic_Impact  \n",
            "0             10.08           14.78              83.25           808.13  \n",
            "1             33.06           23.25              54.02           616.22  \n",
            "2             27.41           65.53              67.78           796.96  \n",
            "3             14.38           87.58              91.39           790.32  \n",
            "4             44.35           88.08              49.61           401.72  \n",
            "...             ...             ...                ...              ...  \n",
            "9995          41.96           10.95              43.41          1483.06  \n",
            "9996           5.45           82.32              59.39           829.61  \n",
            "9997          11.94           26.00              41.46           155.99  \n",
            "9998          44.71           25.07              75.10          1613.90  \n",
            "9999          48.28           98.27              59.38           453.14  \n",
            "\n",
            "[10000 rows x 13 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "url= \"https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/refs/heads/master/data/Hydroponix.csv\"\n",
        "data_climate = pd.read_csv(url)\n",
        "print(data_climate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0ZZWo8XoFb7"
      },
      "source": [
        "## Exercise 1: Data Cleaning & Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OWAKvtFoVnx"
      },
      "source": [
        "The dataset you’ve received contains data from multiple countries, various crops, and inconsistent formatting. Your first job is to narrow the scope of your study and prepare a clean dataset that’s ready for analysis.\n",
        "\n",
        "Your mission:\n",
        "\n",
        "*   Filter the dataset to focus only on rice and wheat\n",
        "*   Drop the regional identifiers column\n",
        "*   Group the data by Country, Year and Crop_type but don't put them as index (*Hint: specific parameter of `groupby`*) and calculate the mean of each column\n",
        "*   Change the type of the columns `Country` and `Crop_type`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "P4xZA6U-ldyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23167ae7-cbb7-4c3a-81f4-f259dc8f74aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Corn', 'Wheat', 'Coffee', 'Sugarcane', 'Fruits', 'Rice', 'Barley',\n",
              "       'Vegetables', 'Soybeans', 'Cotton'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "# Question 2: Find the list of crop types produced by SerriFleur\n",
        "data_climate['Crop_Type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "7qhsCq5uG4Jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838c981a-0e85-45bd-bcd0-75ed263d19c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Country  Year Crop_Type  Average_Temperature  Total_Precipitation  \\\n",
            "0    Argentina  1990      Rice              25.2500          1540.486667   \n",
            "1    Argentina  1990     Wheat              10.7550           867.305000   \n",
            "2    Argentina  1991      Rice              14.3175           761.692500   \n",
            "3    Argentina  1992      Rice              14.8650          1190.220000   \n",
            "4    Argentina  1992     Wheat              27.5200          1304.713333   \n",
            "..         ...   ...       ...                  ...                  ...   \n",
            "660        USA  2022     Wheat              23.7650          1656.835000   \n",
            "661        USA  2023      Rice              -2.1100          1730.185000   \n",
            "662        USA  2023     Wheat              22.2000          1058.493333   \n",
            "663        USA  2024      Rice              22.1640          1199.048000   \n",
            "664        USA  2024     Wheat              10.2050          1751.595000   \n",
            "\n",
            "     CO2_Emissions  Crop_Yield  Irrigation_Access  Pesticide_Use  \\\n",
            "0        14.126667    2.686333          58.413333      42.223333   \n",
            "1        18.350000    2.686500          39.830000      16.875000   \n",
            "2        15.115000    2.538000          42.930000      29.302500   \n",
            "3        19.710000    2.794500          57.740000      33.495000   \n",
            "4        20.786667    1.838667          78.210000      30.903333   \n",
            "..             ...         ...                ...            ...   \n",
            "660      15.870000    2.837500          38.615000      27.900000   \n",
            "661       7.550000    2.440000          56.520000      25.205000   \n",
            "662      21.550000    1.686000          53.066667      23.673333   \n",
            "663      23.534000    1.203200          56.272000      11.388000   \n",
            "664      15.510000    1.981000          54.435000      41.565000   \n",
            "\n",
            "     Fertilizer_Use  Soil_Health_Index  Economic_Impact  \n",
            "0         18.710000          72.513333       618.393333  \n",
            "1         75.195000          75.215000       650.170000  \n",
            "2         59.152500          59.955000       749.937500  \n",
            "3         54.680000          40.260000      1190.750000  \n",
            "4         25.186667          53.616667       581.710000  \n",
            "..              ...                ...              ...  \n",
            "660       53.875000          39.430000       593.395000  \n",
            "661       10.305000          68.785000       590.925000  \n",
            "662       59.930000          57.743333       307.573333  \n",
            "663       59.418000          63.456000       442.964000  \n",
            "664       44.850000          67.390000       933.810000  \n",
            "\n",
            "[665 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "# Data cleaning\n",
        "\n",
        "data_climate = data_climate[(data_climate[\"Crop_Type\"] == \"Rice\") | (data_climate[\"Crop_Type\"] == \"Wheat\")]\n",
        "\n",
        "if 'Region' in data_climate.columns:\n",
        "    data_climate = data_climate.drop(columns = \"Region\")\n",
        "data_climate = data_climate.groupby(['Country', 'Year', 'Crop_Type']).mean().reset_index()\n",
        "data_climate['Country'] = data_climate['Country'].astype('category')\n",
        "data_climate['Crop_Type'] = data_climate['Crop_Type'].astype('category')\n",
        "print(data_climate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "A_x2lAOjbKyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02679ae1-2f7f-495c-e0d8-147fd2f2e10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "# Question 3: Find the total number of countries where Serrifleur operates in\n",
        "num_countries = data_climate['Country'].nunique()\n",
        "print(num_countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RjlKX5qnx_T"
      },
      "source": [
        "**Note on Climate Variables per Crop**:\n",
        "\n",
        "> You may notice that climate-related variables such as average temperature and total precipitation vary across different crops, even for the same country and year. This is because each crop is typically grown in different regions within a country, depending on factors like soil conditions, irrigation availability, and climatic suitability. For example, wheat might be cultivated in cooler northern regions while corn is grown in warmer southern areas. Therefore, the recorded climate data reflects regional conditions specific to where each crop is grown, rather than national averages. This is an important consideration when interpreting the results of this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1uuTvNrxs7j"
      },
      "source": [
        "## Exercise 2: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79-w6Jfjx2rO"
      },
      "source": [
        "Before diving into modeling, your supervisor wants a clear picture of the current state of affairs in South America. You’re tasked with visualizing total precipitation level, economic impacts and soil health.\n",
        "\n",
        "Your mission:\n",
        "\n",
        "\n",
        "*   Plot total precipitation evolution over the years for each crop in Argentina\n",
        "*   Create a pie chart that shows the percentage share of the total economic impact for each crop in Brazil\n",
        "*   Create boxplots comparing soil health across crops and all the countries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcwTpzwkyG24"
      },
      "outputs": [],
      "source": [
        "# Question 4: Create the graph of the total precipitation evolution over the years for each crop in Argentina\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrNXxOqVbu_p"
      },
      "outputs": [],
      "source": [
        "# Question 5: Create the pie chart that shows the percentage share of the totale economic impact for each crop in Brazil\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKQrxOLJb1ay"
      },
      "outputs": [],
      "source": [
        "# Question 6: Create the boxplots that compare the soil health index across crops and all the countries\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DER8gy_4-BTE"
      },
      "source": [
        "## Exercise 3: Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrvsCgCH-MLS"
      },
      "source": [
        "SerriFleur aims to classify countries based on their farming practices in order to provide region-specific recommendations. You propose using unsupervised learning to identify distinct fertilizer usage profiles.\n",
        "\n",
        "Your mission:\n",
        "\n",
        "\n",
        "\n",
        "*   For each crop type, use agglomerative clustering on fertilizer usage with 4 clusters, euclidian distance metric and ward linkage\n",
        "*   Analyze whether wheat and rice follow similar cluster pattern\n",
        "*   Plot the dendrogram representation of fertilizer usage, for each crop type, with euclidian distance metric and average linkage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZc_M75iGq_6"
      },
      "outputs": [],
      "source": [
        "# Agglomerative Clustering for Wheat\n",
        "# Question 7: Find the number of values in each clustering label\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7poRM7IRcTQs"
      },
      "outputs": [],
      "source": [
        "# Agglomerative Clustering for Rice\n",
        "# Question 7: Find the number of values in each clustering label\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBEACtFHc4Qn"
      },
      "outputs": [],
      "source": [
        "# Question 8: Hierarchical Clustering for Wheat\n",
        "# Don't forget to reshape the values with numpy\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q5FZoOxc4W2"
      },
      "outputs": [],
      "source": [
        "# Question 8: Hierarchical Clustering for Rice\n",
        "# Don't forget to reshape the values with numpy\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQibWBMzFeEF"
      },
      "source": [
        "## Exercise 4: Simple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rczsQUbZF7B0"
      },
      "source": [
        "A policymaker asks a simple question: \"Does CO₂ have a measurable impact on crop yield for wheat and rice ?\"\n",
        "You decide to test this using a basic regression model.\n",
        "\n",
        "Your mission:\n",
        "\n",
        "*   Plot the heatmap of correlations for numeric columns only.\n",
        "*   Model Crop_Yield as a function of CO2_Emissions (with `test_size = 0.3`, `random_state = 123` and `shuffle=True`).\n",
        "*   Use MinMaxScaler and find the $R^2$, the MSE and the MAE.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGG4PSFS4IR7"
      },
      "outputs": [],
      "source": [
        "# Question 9: Plot heatmap of correlations\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q_obBNVBWkU"
      },
      "outputs": [],
      "source": [
        "# Seperate features and labels\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcN1pFtABWm9"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTRoLaGGBWqB"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeZuEedGBWsu"
      },
      "outputs": [],
      "source": [
        "# Create linear regression model and fit into the training data\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRng8tlLLXX_"
      },
      "outputs": [],
      "source": [
        "# Question 10: Print out the R2, MSE and MAE score\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJQWRrGLo4-"
      },
      "source": [
        "## Exercise 5: Multivariate Regression with Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD3XdUftLt7F"
      },
      "source": [
        "Real-life relationships are rarely one-dimensional. You now create a new model to include all the other variables of the dataset, some of which are key climate factors.\n",
        "\n",
        "Your mission:\n",
        "\n",
        "\n",
        "*   Transform the column `Crop_Type` using `LabelEncoder`function and the column `Country` using `One-Hot` encoding\n",
        "*   Build a multivariate regression model using all the variables to predict crop yield  (with `test_size = 0.3`, `random_state = 123` and `shuffle=True`)\n",
        "*   Compare its performance with the previous model of part 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8rTI6rvMGBq"
      },
      "outputs": [],
      "source": [
        "# Seperate features and labels, and transform the columns Crop_Type and Country (don't forget to delete the old columns)\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2XSlS8Negj1"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets, create linear regression model and fit into the training data\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2HkVEyQetPm"
      },
      "outputs": [],
      "source": [
        "# Question 11: Print out the R2, MSE and MAE score\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoHDJMbEuj5K"
      },
      "source": [
        "# Part 2: Association Rules - Turn movies ratings into bussiness insights!\n",
        "\n",
        "Lights, Camera, Data!\n",
        "\n",
        "Are you a movie fan? I bet you are! Have you ever wondered how streaming platforms like Netflix seem to know exactly which movie you'll love next? You might have heard that they use recommendation algorithms and wondered how they work.\n",
        "\n",
        "In this assignment, you'll explore how **association rules** can be used to develop a simple recommendation algorithm. The idea is simple: by analyzing which movies are frequently watched together, we can uncover patterns and use these insights to suggest movies based on a user's viewing history.\n",
        "\n",
        "Now, let's dive in and make it happen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-bCiqhv0fvp"
      },
      "source": [
        "## Exercise 1: Data Processing\n",
        "\n",
        "We will use a subset of MoviesLens dataset, which consists of user ratings for a list of movies and information about the movies themselves (i.e., title and genres).\n",
        "\n",
        "First, you need to load the ratings data from the URL below, and try to print out the size of the dataset, the number of users who provided ratings, and the number of movies that have been rated.\n",
        "\n",
        "URL = \"https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/refs/heads/master/data/ml/ratings.csv\"\n",
        "\n",
        "The `ratings` dataset contains four columns:\n",
        "\n",
        "- `userID`: The ID of the user.\n",
        "- `movieID`: The ID of the movie being rated.\n",
        "- `rating`: The rating given by the user for the corresponding movie.\n",
        "- `timestamp`: The timestamp when the rating was recorded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CftwBlXA0ppv"
      },
      "outputs": [],
      "source": [
        "df_ratings = # TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmGTFRzN1-5p"
      },
      "outputs": [],
      "source": [
        "# Print the first 5 rows of the dataframe\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU7XXvHV2hmc"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the dataframe\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMzAtZaclexT"
      },
      "outputs": [],
      "source": [
        "# Print the number of ratings available\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9AYnki-2jlw"
      },
      "outputs": [],
      "source": [
        "# Print the number of users who provided ratings\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QVOte9v20ih"
      },
      "outputs": [],
      "source": [
        "# Print the number of movies have been rated\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dso5zAr22_Tv"
      },
      "source": [
        "[texte du lien](https://)Now, we need to transform the dataset into a matrix of size `n_users` × `n_movies`, where each row represents a user's ratings for all movies. If a user has not rated a movie, the rating will be 0.\n",
        "\n",
        "To create this matrix, we will use the `pivot` function from pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro1m0SFh3qd9"
      },
      "outputs": [],
      "source": [
        "df_ratings_pivot = df_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
        "df_ratings_pivot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOdgXSs31tQ"
      },
      "source": [
        "You can see that there are a lot of NaN values because each user rates only a subset of movies (those they have watched). We need to fill these NaN values with 0s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrY5mpFa31IF"
      },
      "outputs": [],
      "source": [
        "# Replace NaN values with 0\n",
        "df_ratings_pivot.fillna(0, inplace=True)\n",
        "df_ratings_pivot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVftYrr84REC"
      },
      "source": [
        "Next, we don’t just want to identify movies that are frequently watched together—we want to focus on the ones that people actually enjoy. To do this, we'll consider a rating of 3 or higher as an indicator that a viewer liked the movie.\n",
        "\n",
        "Therefore, we need to map ratings ≥ 3 to `True` (liked) and ratings below 3 to `False` (not liked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfa6Y98p4Q1R"
      },
      "outputs": [],
      "source": [
        "df_ratings_pivot = df_ratings_pivot.map(lambda x: True if x >= 3 else False)\n",
        "df_ratings_pivot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLBs1CBq5Fqq"
      },
      "source": [
        "Now, we are ready to apply the association rules!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOE7lwgH5K29"
      },
      "source": [
        "## Exercise 2: Association Rules\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfxMhg1W5SbG"
      },
      "source": [
        "We will apply the **Apriori algorithm** to identify interesting association rules.\n",
        "\n",
        "First, we need to find the frequent itemsets using the following configurations:\n",
        "\n",
        "`min_support = 0.1`\n",
        "`max_len = 2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICFUSwuL5O5r"
      },
      "outputs": [],
      "source": [
        "# Find the frequent itemsets\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9bM-Qy5vU24"
      },
      "outputs": [],
      "source": [
        "# Question 12: Find the number of itemsets\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDeuwi29DVp5"
      },
      "source": [
        "Now, we will mine the association rules from the frequent itemsets found above using `metric='lift'` and `min_threshold=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpTutS_y5snY"
      },
      "outputs": [],
      "source": [
        "# Find association rules\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WI45Ixu5ulJ"
      },
      "source": [
        "Now, let's map the movie IDs with their titles so we can identify them.\n",
        "\n",
        "First, let's load the `movies` dataset from this URL: \"https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/refs/heads/master/data/ml/ratings.csv\"\n",
        "\n",
        "You will see that the `movies` dataset has 3 columns:\n",
        "- `movieID`: The ID of the movie.\n",
        "- `title`: The title of the movie.\n",
        "- `genres`: The genres of the movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-Twult_53le"
      },
      "outputs": [],
      "source": [
        "df_movies = pd.read_csv(\"https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/refs/heads/master/data/ml/movies.csv\")\n",
        "df_movies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex2GS92bxuuO"
      },
      "source": [
        "In the following cell, we provide the code to map movie IDs with their titles. We assume that you are using the variable `rules` to store the association rules. If you used a different variable name, please update the code accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2udb46m5_Ew"
      },
      "outputs": [],
      "source": [
        "def get_movie_name(movie_id):\n",
        "    return df_movies[df_movies['movieId'] == movie_id]['title'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhrp7hCK575C"
      },
      "outputs": [],
      "source": [
        "# Map antecedents and consequents with movie names\n",
        "rules['antecedents'] = rules['antecedents'].apply(lambda x: get_movie_name(list(x)[0]))\n",
        "rules['consequents'] = rules['consequents'].apply(lambda x: get_movie_name(list(x)[0]))\n",
        "rules.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TzzpnwzmVX"
      },
      "source": [
        "Now, we are ready to find out the most interesting association rules (i.e a pair of movies that are watched together) in term of the `lift` metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1gyrL9T6BSl"
      },
      "outputs": [],
      "source": [
        "# Question 13: Find the most interesting rule\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrLc1z998OhB"
      },
      "outputs": [],
      "source": [
        "# Question 14: Find the movie that is the most frequently watched together with \"Beauty and the beast (1991)\"\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZBY_arrLcWk"
      },
      "source": [
        "# Part 3: Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkPr8HtvZZYN"
      },
      "source": [
        "You are working at a leading Tour & Travel Company, and there’s trouble on the horizon — some customers are slipping away, never to book again! Your job is to predict which customers are at risk of churning and help the company keep them engaged.\n",
        "\n",
        "Using historical customer data, you’ll use binary classification algorithms to identify potential churners before they disappear.\n",
        "\n",
        "First, you need to load the dataset from this URL: \"https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/refs/heads/master/data/travel.csv\"\n",
        "\n",
        "The dataset consists of following columns:\n",
        "- `Age`: Age of customer.\n",
        "- `FrequentFlyer`: Whether the customer takes frequent flights.\n",
        "- `AnnualIncomeClass`: Class of the customer's annual income.\n",
        "- `ServicesOpted`: Number of times services opted during recent years.\n",
        "- `AccountSyncedToSocialMedia`: Whether company account of the user is synchronised to their social media.\n",
        "- `BookedHotelOrNot`: Whether the customer books hotels using company services.\n",
        "- `Target`: Whether the customer is a churner (1 for yes, 0 for no).\n",
        "\n",
        "Try to print out the number of samples available in the dataset and types of each columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAx4x_uEZZYP"
      },
      "outputs": [],
      "source": [
        "df_travel = # TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2srwkkniZZYP"
      },
      "outputs": [],
      "source": [
        "# Print the first 5 rows\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ddWtTexZZYP"
      },
      "outputs": [],
      "source": [
        "# Print the number of samples in the dataset\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeaTRGBLZZYQ"
      },
      "outputs": [],
      "source": [
        "# Print the column types\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv1s78JGZZYQ"
      },
      "source": [
        "## Exercise 1: Preprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CWXqxFKZZYQ"
      },
      "source": [
        "First, we want to convert the columns with an object data type to\n",
        "a **categorical** one and visualize the correlation heatmap of the dataset's features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnwfIbXJZZYQ"
      },
      "outputs": [],
      "source": [
        "for col in df_travel.select_dtypes(include=['object']).columns:\n",
        "    df_travel[col] = pd.Categorical(df_travel[col]).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMkt1Ek2ZZYQ"
      },
      "outputs": [],
      "source": [
        "# Question 15: Plot heatmap of correlations\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H5CmDzvPS8r"
      },
      "source": [
        "Now, let's separate the dataset into features (X) and labels (y). Then, split the data into train and test sets using `train_test_split` function from Sklearn with `test_size=0.2`, `random_state=42`, and set `stratify=y` to preserve the label distribution in both sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FyAxE8XZZYQ"
      },
      "outputs": [],
      "source": [
        "# Seperate the dataset into features and labels\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5odTcusZZYQ"
      },
      "outputs": [],
      "source": [
        "# Split into train and test sets\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEKa3hs8ZZYQ"
      },
      "source": [
        "Before building any model, it's useful to establish a simple reference point. Imagine predicting the most frequent class for every single observation—no learning, just repetition. The accuracy you'd get from that naive approach is called the baseline accuracy. It’s the minimum benchmark any meaningful model should aim to outperform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81FKYwfGVcEi"
      },
      "outputs": [],
      "source": [
        "# Question 16: Calculate baseline accuracy\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_dCOOSkVlE9"
      },
      "source": [
        "Now, let's standardize the features using `StandardScaler` from Sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsXoz_P4ZZYR"
      },
      "outputs": [],
      "source": [
        "# Standardise features\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh7QqELYZZYR"
      },
      "source": [
        "## Exercise 2: Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEwks1h_WS-A"
      },
      "source": [
        "First, try use logistic regression for use our data. Use LogisticRegression with the following setting:\n",
        "`penalty='l2', solver='lbfgs', max_iter=1000, random_state=30`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuBxn9IqZZYR"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ODkCfEZZYR"
      },
      "outputs": [],
      "source": [
        "# Question 17: Find the test accuracy\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh_52S9-ZZYR"
      },
      "source": [
        "Next, train a logistic regression classifier with cross-validation. Use 5 folds and 9 values in the logscale between $10^{-4}$ and $10^4$ for the regularizer parameter. For the rest of the arguments, we use the same values as we used for the logistic regression with no cross validation.\n",
        "\n",
        "__Important:__ Set the following values for the arguments:\n",
        "`\n",
        "penalty='l2', solver='lbfgs', max_iter=1000, random_state=42, cv=4, Cs=*9*\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exDVJgXpZZYR"
      },
      "outputs": [],
      "source": [
        "# Train a logistic regression classifier with cross-validation\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU3uVRbJZZYR"
      },
      "outputs": [],
      "source": [
        "# Question 18: Find the value of the regularization parameter and the test accuracy\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP8DzZbSZZYR"
      },
      "source": [
        "## Exercise 3: KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN_YZqG-bQHb"
      },
      "source": [
        "Use the KNeighborClassifier from sklearn using the following setting:\n",
        "`n_neighbors=7, p=2, weights='uniform', algorithm=\"kd_tree\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rLsdM44ZZYS"
      },
      "outputs": [],
      "source": [
        "# KNeighborClassifier\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYk2lZawbcd-"
      },
      "source": [
        "Use `GridSearchCV` to find the best hyper-parameters for your KNN model. Use values between 1 and 10 for `n_neighbors` argument and use either 'uniform' or 'distance' for the `weights` argument. Use a 5-fold cross validation. This translate to the following values for the arguments of the `GridSearchCV`:\n",
        "```\n",
        "param_grid={\"n_neighbors\": range(1, 11), \"weights\":[\"uniform\", \"distance\"]}, cv=5\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNCfOnC7ZZYS"
      },
      "outputs": [],
      "source": [
        "# Question 19: Find the accuracy of the best KNN model\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXxNNB8D1dRN"
      },
      "outputs": [],
      "source": [
        "# Question 20: Find the 'n_neighbors' and 'weights' values for the best KNN model\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCOwJWA2ZZYS"
      },
      "source": [
        "## Exercise 4: Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g633fLfGcpze"
      },
      "source": [
        "Train a Decision Tree classifier on the training data. Use the following values for the arguments of the decision tree:\n",
        "```\n",
        "criterion=\"gini\", max_depth=3, random_state=30\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdm9MBY0ZZYS"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwZfFUKRZZYS"
      },
      "source": [
        "Use `GridSearchCV` to find the best hyper-parameters for your decision tree model. Use values between 1 and 7 for `max_depth` argument and use either 'gini' or 'entropy' for the `criterion` argument. Use a 5-fold cross validation. This translate to the following values for the arguments of the `GridSearchCV`:\n",
        "```\n",
        "param_grid={\"max_depth\": range(1, 8), \"criterion\":[\"gini\", \"entropy\"]}, cv=5\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9PMYnAzZZYS"
      },
      "outputs": [],
      "source": [
        "# Question 21: Find the accuracy of the best decision tree model\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzv6LpnLZZYn"
      },
      "outputs": [],
      "source": [
        "# Question 22: Find the 'criterion' and 'max_depth' values for the best decision tree model\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfT4Z_R8a_h0"
      },
      "source": [
        "The company places a strong emphasis on keeping its customers and has solid resources dedicated to retention. In such a case, choosing the right evaluation metric becomes crucial. The answer lies in understanding what matters most in this context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcAiY9-fPxi"
      },
      "source": [
        "Question 23: Find the best metric for the company\n",
        "\n",
        "> TODO: Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ADWjoMyiII"
      },
      "outputs": [],
      "source": [
        "# Question 24: Find the model that gives the best result using the metric above\n",
        "\n",
        "# TODO: Your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}